diff --git a/driver.cpp b/driver.cpp
index f3f830c..8811a5d 100644
--- a/driver.cpp
+++ b/driver.cpp
@@ -29,6 +29,7 @@
 #include "mlir/Dialect/Vector/IR/VectorOps.h"
 #include "mlir/Dialect/LLVMIR/LLVMDialect.h"
 #include "mlir/Dialect/Affine/IR/AffineOps.h"
+#include "mlir/Dialect/Affine/Passes.h"
 #include "mlir/Dialect/MemRef/IR/MemRef.h"
 #include "mlir/Dialect/Transform/IR/TransformOps.h"
 #include "mlir/Dialect/Transform/DebugExtension/DebugExtensionOps.h"
@@ -74,6 +75,36 @@
 
 using namespace mlir;
 
+LogicalResult applyTransformFromModule(ModuleOp payloadModule, ModuleOp transformModule) {
+    // 查找transform.sequence操作
+    transform::SequenceOp sequenceOp;
+    for (Operation &op : transformModule.getBody()->getOperations()) {
+        if (auto sequence = dyn_cast<transform::SequenceOp>(op)) {
+            sequenceOp = sequence;
+            break;
+        }
+    }
+    
+    if (!sequenceOp) {
+        llvm::errs() << "未在变换模块中找到transform.sequence操作\n";
+        return failure();
+    }
+    
+    // 应用变换
+    transform::TransformOptions options;
+    if (failed(transform::applyTransforms(
+        payloadModule,      // payload root
+        sequenceOp,         // transform operation
+        {},                 // extra mapping
+        options             // options
+    ))) {
+        llvm::errs() << "变换应用失败\n";
+        return failure();
+    }
+    
+    return success();
+}
+
 func::FuncOp createMatMulFunction(OpBuilder &builder, ModuleOp module) {
     // Create the types we need
     // const int64_t M = 1536;
@@ -483,6 +514,24 @@ LogicalResult applyOptimizationPasses(ModuleOp module, MLIRContext &context) {
         module->dump();
     };
 
+    pm.addPass(createCSEPass());
+    pm.addPass(createCanonicalizerPass());
+    pm.addPass(createCSEPass());
+    if (failed(pm.run(module))) {
+        llvm::errs() << "Failed to run CanonicalizerPass\n";
+        return failure();
+    }
+    dumpAfterPass("Canonicalize", module);
+    pm.clear();
+
+    // pm.addNestedPass<func::FuncOp>(mlir::affine::createSimplifyAffineStructuresPass());
+    // if (failed(pm.run(module))) {
+    //     llvm::errs() << "Failed to run SimplifyAffineStructuresPass\n";
+    //     return failure();
+    // }
+    // dumpAfterPass("SimplifyAffineStructures", module);
+    // pm.clear();
+
     // Staticize TensorEmpty
     pm.addNestedPass<func::FuncOp>(createStaticizeTensorEmptyPass());
     if (failed(pm.run(module))) {
@@ -576,12 +625,20 @@ LogicalResult applyOptimizationPasses(ModuleOp module, MLIRContext &context) {
     
     // Loop Hoisting
     pm.addNestedPass<func::FuncOp>(bufferization::createBufferLoopHoistingPass());
-    // if (failed(pm.run(module))) {
-    //     llvm::errs() << "Failed to run BufferLoopHoistingPass\n";
-    //     return failure();
-    // }
-    // dumpAfterPass("Buffer Loop Hoisting", module);
-    // pm.clear();
+    if (failed(pm.run(module))) {
+        llvm::errs() << "Failed to run BufferLoopHoistingPass\n";
+        return failure();
+    }
+    dumpAfterPass("Buffer Loop Hoisting", module);
+    pm.clear();
+
+    pm.addPass(createCanonicalizerPass());
+    if (failed(pm.run(module))) {
+        llvm::errs() << "Failed to run CanonicalizerPass\n";
+        return failure();
+    }
+    dumpAfterPass("Canonicalize", module);
+    pm.clear();
 
     // // Fold Memref
     // pm.addPass(memref::createFoldMemRefAliasOpsPass());
@@ -919,32 +976,45 @@ int main(int argc, char* argv[]) {
         return success();
     });
 
-    // // 解析MLIR文件
-    // OwningOpRef<ModuleOp> module = parseSourceFile<ModuleOp>("input.mlir", &context);
-    // if (!module) {
-    //     llvm::errs() << "解析失败\n";
-    //     return 1;
-    // }
-
     // // 打印模块
     // module->print(llvm::outs());
 
-    // 4.生成 MLIR
+    // // 4.生成 MLIR
     OpBuilder builder(&context);
-    ModuleOp module = ModuleOp::create(LOC);
+    // ModuleOp module = ModuleOp::create(LOC);
 
-    // 创建矩阵乘法函数
-    createMatMulFunction(builder, module);
+    // 解析MLIR文件
+    OwningOpRef<ModuleOp> module = parseSourceFile<ModuleOp>("test/input.mlir", &context);
+    if (!module) {
+        llvm::errs() << "解析失败\n";
+        return 1;
+    }
     llvm::outs() << "\n=== Original MLIR ===\n";
     module->dump();
 
+    // 解析变换IR文件
+    OwningOpRef<ModuleOp> transformModule = parseSourceFile<ModuleOp>("test/transform.mlir", &context);
+    if (!transformModule) {
+        llvm::errs() << "解析变换IR文件失败\n";
+        return 1;
+    }
+    transformModule->dump();
+
+    // // 创建矩阵乘法函数
+    // createMatMulFunction(builder, *module);
+    // llvm::outs() << "\n=== Original MLIR ===\n";
+    // module->dump();
+
     // 进行调度
-    createAndApplyTransform(module);
+    // createAndApplyTransform(*module);
+    if (failed(applyTransformFromModule(*module, *transformModule))) {
+        return 1;
+    }
     llvm::outs() << "\n=== After Schedule ===\n";
     module->dump();
 
     // 使用pass进行优化
-    applyOptimizationPasses(module, context);
+    applyOptimizationPasses(*module, context);
     // llvm::outs() << "\n=== After Optimization ===\n";
     // module->dump();
 
@@ -952,13 +1022,13 @@ int main(int argc, char* argv[]) {
     // llvm::outs() << "\n=== After Schedule2 ===\n";
     // module->dump();
 
-    lowerToLLVM(module, context);
+    lowerToLLVM(*module, context);
     llvm::outs() << "\n=== After Lower To LLVM Dialect ===\n";
     module->dump();
 
     // 翻译到 LLVM IR
     llvm::LLVMContext llvmContext;
-    auto llvmModule = translateToLLVMIR(module, llvmContext);
+    auto llvmModule = translateToLLVMIR(*module, llvmContext);
     if (!llvmModule) {
         llvm::errs() << "Translation to LLVM IR failed.\n";
         return 1;
@@ -966,6 +1036,18 @@ int main(int argc, char* argv[]) {
     llvm::outs() << "\n=== LLVM IR ===\n";
     llvmModule->dump();
 
+    std::error_code EC;
+    llvm::raw_fd_ostream output("./test/kernel.ll", EC);
+    if (EC) {
+        llvm::errs() << "Failed to open output file: " << EC.message() << "\n";
+        return 1;
+    }
+
+    llvm::outs() << "\n=== Writing LLVM IR to kernel.ll ===\n";
+    llvmModule->print(output, nullptr);
+    output.close();
+    llvm::outs() << "LLVM IR successfully written to kernel.ll\n";
+
     // // 应用LLVM优化
     // llvm::OptimizationLevel optLevel = llvm::OptimizationLevel::O3;
     // applyOptimization(*llvmModule, optLevel);
diff --git a/include/Dialect/MTDSP/IR/MTDSPBase.td b/include/Dialect/MTDSP/IR/MTDSPBase.td
index 3105e5c..a70b7a6 100644
--- a/include/Dialect/MTDSP/IR/MTDSPBase.td
+++ b/include/Dialect/MTDSP/IR/MTDSPBase.td
@@ -49,7 +49,7 @@ def MTDSP_Dialect : Dialect {
 //        ↓
 // AddressSpaceAttr (最终属性定义)
 
-def AddressSpaceGlobal : I32EnumAttrCase<"Global", 0, "global">;
+def AddressSpaceGlobal : I32EnumAttrCase<"Global", 0, "ddr">;
 def AddressSpaceWorkgroup : I32EnumAttrCase<"Workgroup", 1, "gsm">;
 def AddressSpaceScalar : I32EnumAttrCase<"Scalar", 2, "sm">;
 def AddressSpaceVector : I32EnumAttrCase<"Vector", 3, "am">;
diff --git a/include/Dialect/MTDSP/IR/MTDSPOps.td b/include/Dialect/MTDSP/IR/MTDSPOps.td
index 5a87ac6..ef1f396 100644
--- a/include/Dialect/MTDSP/IR/MTDSPOps.td
+++ b/include/Dialect/MTDSP/IR/MTDSPOps.td
@@ -430,4 +430,25 @@ def MatmulR12C128Op : MTDSP_Op<"matmul_r12c128", [
   }];
 }
 
+//===----------------------------------------------------------------------===//
+// MatmulMicroKernelOp
+//===----------------------------------------------------------------------===//
+
+def MatmulMicroKernelOp : MTDSP_Op<"matmul_micro_kernel", [
+]> {
+  let summary = "matrix multiplication microkernel operation";
+  let description = [{
+  }];
+
+  let arguments = (ins 
+    AnyMemRef:$lhs,
+    AnyMemRef:$rhs, 
+    AnyMemRef:$dst
+  );
+
+  let assemblyFormat = [{
+    $lhs `,` $rhs `,` $dst attr-dict `:` type($lhs) `,` type($rhs) `,` type($dst)
+  }];
+}
+
 #endif // MTDSP_OPS
\ No newline at end of file
diff --git a/lib/Conversion/ConvertToMTDSP/ConvertToMTDSPPass.cpp b/lib/Conversion/ConvertToMTDSP/ConvertToMTDSPPass.cpp
index 18df067..5958a58 100644
--- a/lib/Conversion/ConvertToMTDSP/ConvertToMTDSPPass.cpp
+++ b/lib/Conversion/ConvertToMTDSP/ConvertToMTDSPPass.cpp
@@ -188,8 +188,13 @@ public:
     //     loc, secondHalfLhs, rhs, secondHalfOutput
     // );
 
-    // 创建MatmulR12C128Op
-    rewriter.create<mtdsp::MatmulR12C128Op>(
+    // // 创建MatmulR12C128Op
+    // rewriter.create<mtdsp::MatmulR12C128Op>(
+    //     loc, lhs, rhs, output
+    // );
+
+    // 创建MatmulMicroKernelOp
+    rewriter.create<mtdsp::MatmulMicroKernelOp>(
         loc, lhs, rhs, output
     );
     
diff --git a/lib/Conversion/MTDSPToLLVM/MTDSPToLLVMPass.cpp b/lib/Conversion/MTDSPToLLVM/MTDSPToLLVMPass.cpp
index 97b032a..1c6a7fc 100644
--- a/lib/Conversion/MTDSPToLLVM/MTDSPToLLVMPass.cpp
+++ b/lib/Conversion/MTDSPToLLVM/MTDSPToLLVMPass.cpp
@@ -814,6 +814,54 @@ LogicalResult extractMatmulParams(
   return success();
 }
 
+// 提取 Matmul 微内核操作所需的指针参数的通用工具函数
+LogicalResult extractMatmulPtrParams(
+    ConversionPatternRewriter &rewriter, Location loc,
+    Value lhsDesc, Value rhsDesc, Value dstDesc,
+    SmallVectorImpl<Value> &callOperands) {
+
+  // 1. 提取 lhs (src_a) 相关参数
+  Value lhsAlignedPtr = rewriter.create<LLVM::ExtractValueOp>(loc, lhsDesc, ArrayRef<int64_t>{1});
+  Value lhsOffset = rewriter.create<LLVM::ExtractValueOp>(loc, lhsDesc, ArrayRef<int64_t>{2});
+  Value lhsPtr = rewriter.create<LLVM::GEPOp>(loc,
+    rewriter.getType<LLVM::LLVMPointerType>(),
+    rewriter.getF32Type(),
+    lhsAlignedPtr,
+    lhsOffset,
+    /*inbounds=*/true);
+
+  // 获取 K_data (lhs 的内层维度大小)
+  Value kData = rewriter.create<LLVM::ExtractValueOp>(loc, lhsDesc, ArrayRef<int64_t>{3, 1});
+
+  // 2. 提取 rhs (src_b) 相关参数
+  Value rhsAlignedPtr = rewriter.create<LLVM::ExtractValueOp>(loc, rhsDesc, ArrayRef<int64_t>{1});
+  Value rhsOffset = rewriter.create<LLVM::ExtractValueOp>(loc, rhsDesc, ArrayRef<int64_t>{2});
+  Value rhsPtr = rewriter.create<LLVM::GEPOp>(loc,
+    rewriter.getType<LLVM::LLVMPointerType>(),
+    rewriter.getF32Type(),
+    rhsAlignedPtr,
+    rhsOffset,
+    /*inbounds=*/true);
+
+  // 3. 提取 dst (dst_c) 相关参数
+  Value dstAlignedPtr = rewriter.create<LLVM::ExtractValueOp>(loc, dstDesc, ArrayRef<int64_t>{1});
+  Value dstOffset = rewriter.create<LLVM::ExtractValueOp>(loc, dstDesc, ArrayRef<int64_t>{2});
+  Value dstPtr = rewriter.create<LLVM::GEPOp>(loc,
+    rewriter.getType<LLVM::LLVMPointerType>(),
+    rewriter.getF32Type(),
+    dstAlignedPtr,
+    dstOffset,
+    /*inbounds=*/true);
+
+  // 4. 按顺序添加所有参数
+  callOperands.push_back(lhsPtr);    // src_a
+  callOperands.push_back(rhsPtr);    // src_b
+  callOperands.push_back(dstPtr);    // dst_c
+  callOperands.push_back(kData);     // K_data
+
+  return success();
+}
+
 //===----------------------------------------------------------------------===//
 // MTDSPToLLVM RewritePatterns: MatmulR6C96Op
 //===----------------------------------------------------------------------===//
@@ -991,6 +1039,63 @@ public:
   }
 };
 
+//===----------------------------------------------------------------------===//
+// MTDSPToLLVM RewritePatterns: MatmulMicroKernelOp
+//===----------------------------------------------------------------------===//
+
+class MatmulMicroKernelOpLowering : public OpConversionPattern<mtdsp::MatmulMicroKernelOp> {
+  using OpConversionPattern<mtdsp::MatmulMicroKernelOp>::OpConversionPattern;
+public:
+  LogicalResult
+  matchAndRewrite(mtdsp::MatmulMicroKernelOp op, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    auto loc = op.getLoc();
+    auto module = op->getParentOfType<ModuleOp>();
+    
+    // 1. 检查函数是否已经声明
+    if (!module.lookupSymbol("matmul_micro_kernel")) {
+      // 声明函数类型
+      SmallVector<Type, 6> inputTypes = {
+          rewriter.getType<LLVM::LLVMPointerType>(),      // float* src_a
+          rewriter.getType<LLVM::LLVMPointerType>(),      // lvector float* src_b 
+          rewriter.getType<LLVM::LLVMPointerType>(),      // lvector float* dst_c
+          rewriter.getI64Type(),                          // const long K_data
+      };
+      auto functionType = FunctionType::get(op.getContext(), 
+                                          inputTypes, 
+                                          /*results=*/{}); // void返回类型
+      
+      // 在模块开始处创建函数声明
+      OpBuilder::InsertionGuard guard(rewriter);
+      rewriter.setInsertionPointToStart(module.getBody());
+      rewriter.create<func::FuncOp>(
+          module->getLoc(),
+          "matmul_micro_kernel",     // 函数名
+          functionType               // 函数类型
+      ).setPrivate();                // 设置为私有
+    }
+
+    // 2. 提取所有参数
+    SmallVector<Value, 6> callOperands;
+    if (failed(extractMatmulPtrParams(rewriter, loc, 
+        adaptor.getLhs(), adaptor.getRhs(), adaptor.getDst(), 
+        callOperands))) {
+      return failure();
+    }
+
+    // 3. 创建函数调用
+    rewriter.create<func::CallOp>(
+      loc,
+      TypeRange{},              // 无返回值
+      "matmul_micro_kernel",    // callee
+      callOperands);            // 参数列表
+
+    // 4. 由于原操作没有返回值,直接擦除原操作即可
+    rewriter.eraseOp(op);
+    return success();
+  }
+};
+
 namespace {
 struct MTDSPToLLVMConversionPass : public PassWrapper<MTDSPToLLVMConversionPass, OperationPass<ModuleOp>> {
   MLIR_DEFINE_EXPLICIT_INTERNAL_INLINE_TYPE_ID(MTDSPToLLVMConversionPass)
@@ -1051,7 +1156,8 @@ void MTDSPToLLVMConversionPass::runOnOperation() {
         SetPrirOpLowering,
         MatmulR6C96OpLowering,
         MatmulR6C128OpLowering,
-        MatmulR12C128OpLowering
+        MatmulR12C128OpLowering,
+        MatmulMicroKernelOpLowering
     >(
       typeConverter, 
       context);
diff --git a/lib/Dialect/Schedule/Transforms/MultiBufferPass.cpp b/lib/Dialect/Schedule/Transforms/MultiBufferPass.cpp
index 626c245..151d976 100644
--- a/lib/Dialect/Schedule/Transforms/MultiBufferPass.cpp
+++ b/lib/Dialect/Schedule/Transforms/MultiBufferPass.cpp
@@ -13,7 +13,7 @@
 
 using namespace mlir;
 
-// #define USE_DMA_OPT 1
+#define USE_DMA_OPT 1
 
 namespace
 {
@@ -44,6 +44,7 @@ namespace
     void convertCopyToDMA(linalg::CopyOp copyOp, OpBuilder &builder) {
       builder.setInsertionPoint(copyOp);
       
+#if USE_DMA_OPT
       // 为 CopyOp 创建一个 channel 常量
       auto channelConst = builder.create<arith::ConstantOp>(
           copyOp.getLoc(),
@@ -56,7 +57,6 @@ namespace
           builder.getI32Type(),
           channelConst);
 
-#if USE_DMA_OPT
       auto dmaOp = builder.create<mtdsp::DMAOptOp>(
           copyOp.getLoc(),
           copyOp.getInputs()[0],    // 输入操作数
@@ -404,6 +404,7 @@ namespace
         OpBuilder &builder) {
       llvm::SmallVector<Value> channels;
 
+#if USE_DMA_OPT
       // 创建常量2
       auto c2 = builder.create<arith::ConstantOp>(
           builder.getUnknownLoc(),
@@ -421,10 +422,11 @@ namespace
           builder.getUnknownLoc(),
           diviOp.getResult(),
           c2);
+#endif
 
       // 使用映射为readCopyOps中的每个CopyOp创建对应的DMAOp
       for (linalg::CopyOp copyOp : readCopyOps) {
-
+#if USE_DMA_OPT
         Value inputChannel = builder.create<arith::AddIOp>(
             copyOp.getLoc(),
             copyOpsChannelStart[copyOp],
@@ -438,7 +440,6 @@ namespace
             inputChannel           // 源值
         );
 
-#if USE_DMA_OPT
         auto channel = builder.create<mtdsp::DMAOptOp>(
             copyOp.getLoc(),
             prefetchMapping.lookup(copyOp.getInputs()[0]),
@@ -651,7 +652,7 @@ namespace
           }
           bool isWriteCopy = llvm::is_contained(writeCopyOps, currentCopyOp);
           if (isWriteCopy) {
-
+#if USE_DMA_OPT
             // 创建常量2
             auto c2 = builder.create<arith::ConstantOp>(
                 currentCopyOp.getLoc(),
@@ -683,7 +684,6 @@ namespace
                 inputChannel           // 源值
             );
 
-#if USE_DMA_OPT
             auto channel = builder.create<mtdsp::DMAOptOp>(
                 currentCopyOp.getLoc(),
                 currentMapping.lookup(currentCopyOp.getInputs()[0]),
